# Representation

1. 贝叶斯定理：**已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P\(B\|A\)的情况下如何求得P\(A\|B\)**。

  因为事件A和B同时发生的概率为（在A发生的情况下发生B）或者（在B发生的情况下发生A）：$$P(A \cap B) = P(A)*P(B|A) = P(B)*P(A|B)$$

  那么可以得到：$$P(A|B)=\frac{P(B|A)*P(A)}{P(B)}$$

  由此得到启示，可以利用贝叶斯定理进行分类，**对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别**。

2. 先验概率与后验概率

  先验概率P\(Y\)：就是在事情尚未发生之前，我们对该事件发生概率的估计，是**根据以往经验分析得到的概率**。**J往往这个具体事情还未发生前，普遍的事情调查，比如平日的堵车概率，下雨的概率等**。

  后验概率P\(Y\|X\)：给定X时，Y发生的概率。是表示**在某事件已经发生的条件下，求该事件由某个元素引起的可能性大小**。**J后验概率本身是一个条件概率，但是特指相对于先验概率而言，有一个元素影响下，讨论后验概率。**

3. 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入\/输出的联合概率分布；然后基于此模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

  输入: 线性可分训练集$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$, 其中$$x_i\in{\scr {X}}=R^n, y_i\in{\scr {Y}}={c_1,\cdots,c_K}$$，$$X,Y$$分别是定义在$${\scr{X,Y}}$$上的随机向量和随机变量。假设$$x^{(j)}$$可取值有$$S_j$$个。**注意：随机变量的意义在于产生一系列样本，因为不能说样本属于某集合。**$$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T ,x_i^{(j)}=(a_{j1},a_{j2},\cdots,a_{jS_j})$$

  先验概率分布：$$P(Y=c_k), k = 1,2,...,K$$

  条件概率分布：$$P(X=x\mid Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\\stackrel{条件独立性}=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$

  后验概率分布：$$P(Y=c_k\mid X=x)=\frac{P(X=X\mid Y=c_k)P(Y=c_k)}{P(X)}\\ =\frac{P(X=x\mid Y=c_k)P(Y=c_k)}{\sum_kP(X=x\mid Y=c_k)P(Y=c_k)}\\\stackrel{条件独立性}=\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}\mid Y=c_k)}$$

  后验概率最大化：$$ y=arg\min_{c_k}\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}\mid Y=c_k)}\
  \\=arg\min{c_k}{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}$$

  在上式中，因为分母对于所有的c\_k都是相同的，所以可以只求分子的极值。**记住该模型。**


# Evalution

朴素贝叶斯法将实例分到**后验概率最大的类中，这等价于期望经验风险最小化**。 **记住该策略**。具体推导可见《统计学习方法》P48。

# Optimization

1. 判别模型与生成模型：**建模及优化的对象不一样，一个是条件概率，一个是联合概率。**

  1. 判别模型：

    简单的说就是分类的最终结果是以某个函数或者是假设函数的取值范围来表示它属于那一类的，例如H\(x\)&gt;0就是第一类，H\(x\)&lt;0。**判别模型主要对p\(y\|x\)建模（优化条件概率分布），通过x来预测y**。在建模的过程中不需要关注联合概率分布。只关心如何优化p\(y\|x\)使得数据可分。通常，判别模型在分类任务中的表现要好于生成模型。但判别模型建模过程中通常为有监督的，而且难以被扩展成无监督的。比如决策树、逻辑回归、SVM等。

  2. 生成模型：

    该模型对观察序列的**联合概率分布p\(x,y\)建模（优化训练数据的联合分布概率）**，在获取联合概率分布之后，可以通过贝叶斯公式得到条件概率分布。生成式模型所带的信息要比判别模型更丰富。除此之外，生成模型较为容易的实现增量学习。比如朴素贝叶斯等。

2. 朴素贝叶斯分类器的**事件模型（event model）：多项式模型、高斯模型、伯努利模型**。

  所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计和贝叶斯估计。

  **类的先验概率P\(Y\)**可以通过假设各类等概率来计算（先验概率 = 1 \/ \(类的数量\)），或者**通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）\/\(样本总数\)）**。

  **对于类条件概率P\(X\|Y\)来说，直接根据样本出现的频率来估计会很困难。在现实应用中样本空间的取值往往远远大于训练样本数，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计P\(X\|Y\)不可行，因为"未被观察到"和"出现概率为零"是不同的。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。**

  1. 多项式模型：**当特征是离散的时候，使用多项式模型**。具体见下文的参数估计方法。

  2. 高斯模型：当特征是连续变量的时候，运用多项式模型就会导致很多$$P(X^{(j)}=x^{(j)}|Y=c_k)=0$$（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。**所以处理连续的特征变量，应该采用高斯模型**。

    以下是一组人类身体特征的统计资料，已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？

    ![](/assets/gaussian model example.PNG)

    根据朴素贝叶斯分类器，计算这个式子的值：`P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别)`

    这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？

    这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。

    比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。

    ![](/assets/gaussian model example2.PNG)

    对于脚掌和体重同样可以计算其均值与方差。有了这些数据以后，就可以计算性别的分类了。

    ```
    P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男) 
    　　　　= 6.1984 x e-9
    　　P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女) 
    　　　　= 5.3778 x e-4
    ```

    可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。

    总结：**高斯模型假设每一维特征都服从高斯分布（正态分布）**：

    $$P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi\sigma_{y_{k},i}^{2}}}e^{-\frac{(x_{i}-\mu_{y_{k},i})^{2}}{2  \sigma_{y_{k},i}^{2}}}$$

    $$\mu_{y_{k},i}$$表示类别为$$y_{k}$$的样本中，第i维特征的均值。

    $$\sigma_{y_{k},i}^{2}$$表示类别为$$y_{k}$$的样本中，第i维特征的方差。

  3. 伯努利模型：与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0\(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0\)。

    伯努利模型中，条件概率$$P(x_{i}|y_{k})$$的计算方式是：

    当特征值$$x_{i}$$为1时，$$P(x_{i}|y_{k})=P(x_{i}=1|y_{k})$$；

    当特征值$$x_{i}$$为0时，$$P(x_{i}|y_{k})=1-P(x_{i}=1|y_{k})$$；

3. 在朴素贝叶斯法中，学习意味着估计$$P(Y=c_k)$$和$$P(X^{(j)}=x^{(j)}|Y=c_k)$$。由上述分析可知，可以运用极大似然估计和贝叶斯估计来估计相应的概率。**这里给出多项式模型的情况，高斯模型并不要用极大似然估计！注意其中I函数在两者相等时取1，两者不等时取0**。**记住此优化就是利用极大似然或贝叶斯来估计参数。**

  1. 极大似然估计：《统计学习方法》中直接给出了得到的答案。

    $$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}$$

    $$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}=a_{jl}|y_i=c_k)}{N}$$

    这里搜索了网上的证明方法，给出启示：**记住该估计方法：XXX的极大似然估计，就是将XXX当作参数来进行估计，所以需要将各个样本出现概率的连乘积表示为该参数的函数！样本的联合分布（当X是连续型随机变量时为概率密度，当X为离散型随机变量时为概率分布）称为似然函数。**联合分布(joint distribution)描述了多个随机变量的概率分布。

    **约定**$$P(x;θ)$$表示$$θ$$为**非随机变量，只是未知常数**。而$$P(x,θ)$$中的$$θ$$表示为随机变量。
    
    **约定**$$P(x|θ)$$表示条件概率，当不表示条件概率的时候，与$$P(x;θ)$$等价；而$$P(x;θ)$$表示有待估参数，可以直接认为$$P(x)$$。
 
    估计先验概率$$P(c_k)$$，令参数$$P(Y=c_k)=\theta_k$$，其中$$k\in \left\{ 1,2..K \right\}$$。

    那么随机变量Y的概率可以用参数来表示为一个紧凑的形式（**也就是将本来需要列出各个情况的概率用一个式子来表达**），P(Y)=∑k=1KθkI(Y=ck)，I是指示函数Y=ck成立时，I=1；否则I=0。

    极大似然函数$$L(y_1,y_2..y_N;\theta_k)=\prod_{i=1}^{N}P(y_i) =\prod_{k=1}^{K}\theta_k^{N_k}$$，其中N为样本总数，$$N_k$$为样本中$$Y=c_k$$的样本数目。

    取对数得到$$l(\theta_k)=ln(L(\theta))=\sum_{k=1}^{K}{N_k ln\theta_k}$$，要求该函数的最大值，注意到约束条件$$\sum_{k=1}^{K}{\theta_k} =1$$可以用拉格朗日乘子法，即$$l(\theta_k,\lambda)=\sum_{k=1}^{K}{N_k ln\theta_k} +\lambda(\sum_{k=1}^{K}{\theta_k} -1)$$，求导就可以得到：$$\frac{N_k}{\theta_k}+\lambda=0$$。联立所有的k以及约束条件得到$$\theta_k=\frac{N_k}{N}$$，证明完毕。

  2. 贝叶斯估计：用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。等价于在随机变量各个取值的频数上赋予一个正数$$\lambda$$。常取$$\lambda$$成为拉普拉斯平滑。这里直接给出了得到的答案。

    $$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K\lambda}$$

    $$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}=a_{jl}|y_i=c_k)+\lambda}{N+S_j\lambda}$$

4. 贝叶斯估计：**贝叶斯估计与朴素贝叶斯是不同的概念**。


# Code

```python
#Import Library
from sklearn.naive_bayes import GaussianNB
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object
model = GaussianNB()
# there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link
#from sklearn.naive_bayes import BernoulliNB
;clf = BernoulliNB()
#from sklearn.naive_bayes import MultinomialNB
;clf = MultinomialNB()
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)
```

sklearn中一共有三种模型：MultinomialNB，BernoulliNB和GaussianNB。

用法大致相同，**参数alpha=1.0（当多项式和伯努利时候启用的拉普拉斯平滑算子）；fit\_prior=False（当多项式和伯努利时候启用：是否根据现有数据计算先验概率分布P\(Y\)，如果false，表示启用统一分布；但如果给了class\_prior，那么就不会根据现有数据计算先验概率，高斯中是priors参数）**。

# Reference

* [朴素贝叶斯法](http://www.wengweitao.com/po-su-bei-xie-si-fa.html)
* [贝叶斯网络](http://clyyuanzi.github.io/2016/03/17/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/)
* [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)
* [朴素贝叶斯](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/nb.md)
* [朴素贝叶斯及其数学推导](http://blog.csdn.net/cvrszeng/article/details/52336093)
