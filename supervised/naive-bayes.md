# Representation

1. 贝叶斯定理：**已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P\(B\|A\)的情况下如何求得P\(A\|B\)**。

  因为事件A和B同时发生的概率为（在A发生的情况下发生B）或者（在B发生的情况下发生A）：$$P(A \cap B) = P(A)*P(B|A) = P(B)*P(A|B)$$

  那么可以得到：$$P(A|B)=\frac{P(B|A)*P(A)}{P(B)}$$

  由此得到启示，可以利用贝叶斯定理进行分类，**对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别**。

2. 先验概率与后验概率

  先验概率P\(Y\)：就是在事情尚未发生之前，我们对该事件发生概率的估计，是**根据以往经验分析得到的概率**。**J往往这个具体事情还未发生前，普遍的事情调查，比如平日的堵车概率，下雨的概率等**。

  后验概率P\(Y\|X\)：给定X时，Y发生的概率。是表示**在某事件已经发生的条件下，求该事件由某个元素引起的可能性大小**。**J后验概率本身是一个条件概率，但是特指相对于先验概率而言，有一个元素影响下，讨论后验概率。**

3. 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入\/输出的联合概率分布；然后基于此模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

  输入: 线性可分训练集$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$, 其中$$x_i\in{\scr {X}}=R^n, y_i\in{\scr {Y}}={c_1,\cdots,c_K}$$，$$X,Y$$分别是定义在$${\scr{X,Y}}$$上的随机向量和随机变量。假设$$x^{(j)}$$可取值有$$S_j$$个。**注意：随机变量的意义在于产生一系列样本，因为不能说样本属于某集合。**$$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T ,x_i^{(j)}=(a_{j1},a_{j2},\cdots,a_{jS_j})$$

  先验概率分布：$$P(Y=c_k), k = 1,2,...,K$$

  条件概率分布：$$P(X=x\mid Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)\\\stackrel{条件独立性}=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$

  后验概率分布：$$P(Y=c_k\mid X=x)=\frac{P(X=X\mid Y=c_k)P(Y=c_k)}{P(X)}\\ =\frac{P(X=x\mid Y=c_k)P(Y=c_k)}{\sum_kP(X=x\mid Y=c_k)P(Y=c_k)}\\\stackrel{条件独立性}=\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}\mid Y=c_k)}$$

  后验概率最大化：$$ y=arg\min_{c_k}\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}\mid Y=c_k)}\
  \\=arg\min{c_k}{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}$$

  在上式中，因为分母对于所有的c\_k都是相同的，所以可以只求分子的极值。**记住remember该模型。**


# Evalution

朴素贝叶斯法将实例分到**后验概率最大的类中，这等价于期望经验风险最小化**。 **记住remember该策略**。具体推导可见《统计学习方法》P48。

# Optimization

1. 判别模型与生成模型：**建模及优化的对象不一样，一个是条件概率，一个是联合概率。**

  1. 判别模型：

    简单的说就是分类的最终结果是以某个函数或者是假设函数的取值范围来表示它属于那一类的，例如H\(x\)&gt;0就是第一类，H\(x\)&lt;0。**判别模型主要对p\(y\|x\)建模（优化条件概率分布），通过x来预测y**。在建模的过程中不需要关注联合概率分布。只关心如何优化p\(y\|x\)使得数据可分。通常，判别模型在分类任务中的表现要好于生成模型。但判别模型建模过程中通常为有监督的，而且难以被扩展成无监督的。比如决策树、逻辑回归、SVM等。

  2. 生成模型：

    该模型对观察序列的**联合概率分布p\(x,y\)建模（优化训练数据的联合分布概率）**，在获取联合概率分布之后，可以通过贝叶斯公式得到条件概率分布。生成式模型所带的信息要比判别模型更丰富。除此之外，生成模型较为容易的实现增量学习。比如朴素贝叶斯等。


2. 朴素贝叶斯分类器的**事件模型（event model）：多项式模型、高斯模型、伯努利模型**。

  所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计和贝叶斯估计。

  **类的先验概率P\(Y\)**可以通过假设各类等概率来计算（先验概率 = 1 \/ \(类的数量\)），或者**通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）\/\(样本总数\)）**。

  **对于类条件概率P\(X\|Y\)来说，直接根据样本出现的频率来估计会很困难。在现实应用中样本空间的取值往往远远大于训练样本数，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计P\(X\|Y\)不可行，因为"未被观察到"和"出现概率为零"是不同的。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。**

  1. 多项式模型：**当特征是离散的时候，使用多项式模型**。具体见下文的参数估计方法。

  2. 高斯模型：当特征是连续变量的时候，运用多项式模型就会导致很多$$P(X^{(j)}=x^{(j)}|Y=c_k)=0$$（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。**所以处理连续的特征变量，应该采用高斯模型**。

    以下是一组人类身体特征的统计资料，已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？

    ![](/assets/gaussian model example.PNG)

    根据朴素贝叶斯分类器，计算这个式子的值：`P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别)`

    这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？

    这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。

    比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。

    ![](/assets/gaussian model example2.PNG)

    对于脚掌和体重同样可以计算其均值与方差。有了这些数据以后，就可以计算性别的分类了。

    ```
    P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男) 
    　　　　= 6.1984 x e-9
    　　P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女) 
    　　　　= 5.3778 x e-4
    ```

    可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。

    总结：**高斯模型假设每一维特征都服从高斯分布（正态分布）**：

    $$P(x_{i}|y_{k})=\frac{1}{\sqrt{2\pi\sigma_{y_{k},i}^{2}}}e^{-\frac{(x_{i}-\mu_{y_{k},i})^{2}}{2  \sigma_{y_{k},i}^{2}}}$$

    $$\mu_{y_{k},i}$$表示类别为$$y_{k}$$的样本中，第i维特征的均值。

    $$\sigma_{y_{k},i}^{2}$$表示类别为$$y_{k}$$的样本中，第i维特征的方差。

  3. 伯努利模型：与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，伯努利模型中每个特征的取值只能是1和0\(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0\)。

    伯努利模型中，条件概率$$P(x_{i}|y_{k})$$的计算方式是：

    当特征值$$x_{i}$$为1时，$$P(x_{i}|y_{k})=P(x_{i}=1|y_{k})$$；

    当特征值$$x_{i}$$为0时，$$P(x_{i}|y_{k})=1-P(x_{i}=1|y_{k})$$；


3. 在朴素贝叶斯法中，学习意味着估计$$P(Y=c_k)$$和$$P(X^{(j)}=x^{(j)}|Y=c_k)$$。由上述分析可知，可以运用极大似然估计和贝叶斯估计来估计相应的概率。**这里给出多项式模型的情况，高斯模型并不要用极大似然估计！注意其中I函数在两者相等时取1，两者不等时取0**。**记住remember此优化就是利用极大似然或贝叶斯来估计参数。**

  1. 极大似然估计：《统计学习方法》中直接给出了得到的答案。

    $$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}$$

    $$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}=a_{jl}|y_i=c_k)}{N}$$

    这里搜索了网上的证明方法，给出启示：**记住remember该估计方法：XXX的极大似然估计，就是将XXX当作参数来进行估计，所以需要将各个样本出现概率的连乘积表示为该参数的函数！样本的联合分布（输入与输出对又称为样本）称为似然函数。应该是联合概率分布P\(X,Y\)，而不是单P\(X\)或P\(Y\)，除非是没有类标记，那就用P\(X\)**

    **约定**$$P(x;θ)$$表示$$θ$$为**非随机变量，只是未知常数**。而$$P(x,θ)$$中的$$θ$$表示为随机变量。

    **约定**$$P(x|θ)$$表示条件概率，当不表示条件概率的时候，与$$P(x;θ)$$等价；而$$P(x;θ)$$表示有待估参数，可以直接认为$$P(x)$$。

    把$$p(y=c_{k})$$和$$p(x^{(j)}=a_{jl}|y=c_{k})$$作为参数。

    $$p(y)=\prod_{k=1}^{K} p(y=c_{k} )^{I(y=c_{k})}\\
    p(x|y=c_{k} )=\prod_{j=1}^{n} p(x^{(j)}|y=c_{k})\\
    =\prod_{j=1}^{n}\prod_{l=1}^{S_{j} }  p(x^{(j)}={a_{jl}}|y=c_{k})^{I(x^{(j)}={a_{jl}},y=c_{k})}$$

    为叙述方便起见，下面以$$\varphi$$代表参数集合$${p(y=c_{k}),p(x^{(j)}=a_{jl}|y=c_{k})}$$。

    首先写出log似然函数：

    $$\iota (
    \varphi )=log\prod_{i=1}^{N} p(x_{i} ,y_{i};\varphi)\\
    =log\prod_{i=1}^{N} p(x_{i}|y_{i};\varphi   )p(y_{i};\varphi)\\
    =log\prod_{i=1}^{N} (\prod_{j=1}^{n} p(x_{i}^{(j)} |y_{i} ;\varphi))p(y_{i};\varphi)\\
    =\sum_{i=1}^{N}{(logp(y_{i} ,\varphi )+\sum_{j=1}^{n}{logp(x_{i}^{(j)}|y_{i};\varphi)})}\\ 
    =\sum_{i=1}^{N}{[\sum_{k=1}^{K}{logp(y=c_{k} )}^{I(y_{i}=c_{k})}\\
    +\sum_{j=1}^{n}{\sum_{l=1}^{S_{j}}{logp(x^{(j)} =a_{jl} |y_{i}=c_{k})^{I(x_{i}^{(j)} =a_{jl},y_{i}=c_{k})}}}]}\\ 
    =\sum_{i=1}^{N}{[\sum_{k=1}^{K}{I(y_{i}=c_{k})logp(y=c_{k})}\\
    +\sum_{j=1}^{n}{\sum_{l=1}^{S_{j}}{I(x_{i}^{(j)} =a_{jl},y_{i}=c_{k})logp(x^{(j)} =a_{jl} |y_{i}=c_{k})}}]}$$

    在上式中是把$${p(y=c_{k}),p(x^{(j)}=a_{jl}|y=c_{k}),j=1,2,...,n;l=1,2,...,S_{j};k=1,2,...,K}$$作为参数，有这么多参数，当然因为有$$\sum_{k=1}^{K}{p(y=c_{k})} =1$$等约束，实际参数会少一点。

    **J所以分成了两部分，可以分开讨论！！！！但是始终是从P\(X,Y\)开始讨论开的。**

    那么随机变量Y的概率可以用参数来表示为一个紧凑的形式（**也就是将本来需要列出各个情况的概率用一个式子来表达**），$$P(Y)=\prod_{k=1}^{K} \theta_k^{I(Y=c_k)}$$，I是指示函数Y=ck成立时，I=1；否则I=0。

    极大似然函数$$L(y_1,y_2..y_N;\theta_k)=\prod_{i=1}^{N}P(y_i) =\prod_{k=1}^{K}\theta_k^{N_k}$$，其中N为样本总数，$$N_k$$为样本中$$Y=c_k$$的样本数目。

    取对数得到$$l(\theta_k)=ln(L(\theta))=\sum_{k=1}^{K}{N_k ln\theta_k}$$，要求该函数的最大值，注意到约束条件$$\sum_{k=1}^{K}{\theta_k} =1$$可以用拉格朗日乘子法，即$$l(\theta_k,\lambda)=\sum_{k=1}^{K}{N_k ln\theta_k} +\lambda(\sum_{k=1}^{K}{\theta_k} -1)$$，求导就可以得到：$$\frac{N_k}{\theta_k}+\lambda=0$$。联立所有的k以及约束条件得到$$\theta_k=\frac{N_k}{N}$$，证明完毕。

    估计条件概率$$P(X^{(j)}=a_{jl}|y=c_k)$$，令参数$$P(X^{(j)}=a_{jl}|y=c_k)=\theta_{kjl}$$

  2. 贝叶斯估计：用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。等价于在随机变量各个取值的频数上赋予一个正数$$\lambda$$。常取$$\lambda$$成为拉普拉斯平滑。这里直接给出了得到的答案。

    $$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K\lambda}$$

    $$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}=a_{jl}|y_i=c_k)+\lambda}{N+S_j\lambda}$$


4. 贝叶斯估计：**贝叶斯估计与朴素贝叶斯是不同的概念**。

  极大似然估计的基本想法是：我们所看到的，就是最可能发生的。所以通过最大化实验数据发生的概率$$P(x;\theta)$$（其中参数$$\theta$$是未知的），取极值时对应的$$\hat{\theta}$$即为最大似然估计。

  而**贝叶斯理解的关键在于把事件发生的概率θ看作一个随机变量**，即认为参数也是随机的。首先有公式如下：

  $$P(\theta | x)=\frac{P(x|\theta)P(\theta)}{\sum_\theta P(x|\theta)P(\theta)}$$

  $$\theta$$表示一个事件发生的概率，例如扔一个硬币的结果正面朝上的概率，这个概率$$\theta$$是一个随机变量，$$P(\theta)$$为$$\theta$$的先验分布，“先”表示在实验之前，先验是指获得数据之前对于事件发生概率$$\theta$$的预估，实际上就是预先假定的$$\theta$$的一个概率分布，$$x$$表示实验数据。

   不妨假设$$\theta$$取值空间为$$θ_i$$,1≤i≤N，其中$$\sum_{i=1}^N\theta_i=1$$，将上面的公式写得更加容易理解一点如下：

   $$P(\theta=\theta_i | x)=\frac{P(x|\theta=\theta_i)P(\theta=\theta_i)}{\sum_{i=1}^N P(x|\theta=\theta_i)P(\theta=\theta_i)} ,\hspace{20pt} 1 \leq i \leq N$$

  **在实验之后，利用实验数据对θ的概率分布进行校正，即得到θ的后验分布，然后求期望，最终得到贝叶斯估计，记住remember这个思想以及所有事情都是随机的思想**：

  $$\hat{\theta}=\sum_{i=1}^N \theta_i P(\theta=\theta_i|x)$$

  一般地，对于连续随机变量$$\theta$$，$$\theta$$的贝叶斯估计为：$$\hat{\theta}=\int \theta P(\theta|x) d\theta$$

5. 极大似然估计与贝叶斯估计的例子：**所有的贝叶斯估计，随着样本量的增加先验的影响逐渐减弱，贝叶斯估计趋近极大似然估计，J具体贝叶斯的应用可能不需要搞懂，只要知道效果即可**

  一枚硬币，抛掷出现正面的概率为p，出现反面的概率为1-p，但是参数p未知。为了估计参数 p 的取值，进行 10 次随机试验，出现了3次正面，7次反面。现在我们已经获取了实验数据，问题是如何才能通过实验数据估计出参数p呢？

  _极大似然估计_：

  参数$$\theta$$在该问题中指的是正面出现的概率p，实验数据指的是在10次实验中出现3次正面，7次反面。如果实验中出现h次正面，t次反面，则该实验结果出现的频率为（假设正反面出现的次序确定）：$$P(x|\theta)=p^h(1-p)^t$$

  第二步，极大化上面的式子即可得到p的贝叶斯估计。由于极大化$$P(x|\theta)$$等价于极大化$$\log[P(x|\theta)]$$，所以可以通过求解$$\log[P(x|\theta)]$$的最大值简化求解过程$$\log [P(x|\theta)]=h\log[p]-t\log[1-p]$$

  极值条件为

  $$\frac{\partial \log[P(x|\theta)]}{\partial p}=0$$

  $$\frac{\partial \log[P(x|\theta)]}{\partial p}=\frac hp-\frac t{1-p}=0$$

  求解上式可以得到$$\hat p = \frac h{h+t}$$

  这就是硬币正面出现概率的极大似然估计。将具体的实验结果代入以上公式计算这枚硬币出现的概率为p=3\/10。

  _贝叶斯估计_：

  还是回到硬币的问题，我们通过极大似然估计得到硬币出现正面的概率是3\/10，但是生活经验告诉我们硬币正反面出现的概率相等都是1\/2。到底我们应该相信那个结果呢？一种好的方法就是将生活经验和实验数据两个因素综合在一起考虑，贝叶斯估计很好的做到了这一点。

  **贝叶斯估计可以分为三个步骤来实现。第一步确定先验，第二步写出似然函数并计算后验，第三步根据后验计算贝叶斯估计。**下面通过硬币的例子来说明贝叶斯估计的实现步骤。

  第一步确定先验，我们使用的先验分布是$$p\sim Beta(\alpha, beta)$$，$$Beta(\alpha, \beta)$$具体是这个样子$$f(p;\alpha, \beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}$$

  $$Beta(\alpha, \beta)$$的含义是实验之前已经进行了$$\alpha + \beta$$次扔硬币的实验，出现了$$\alpha$$次正面和$$\beta$$次反面。

   第二步写出似然函数，并计算后验。

   $$P(\theta|x)\propto P(x|\theta)P(\theta)$$

  $$P(x|\theta)P(\theta)=p^h(1-p)^t\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}$$

  添加归一化系数（保证$$\int_\theta P(x|\theta)P(\theta)=1$$）之后可以得到$$P(\theta|x)=p^h(1-p)^t\frac{\Gamma(\alpha+\beta+h+t)}{\Gamma(\alpha+h) \Gamma(\beta+t)}p^{\alpha+h-1}(1-p)^{\beta+t-1}$$即$$P(\theta|x)\sim Beta(\alpha+h, \beta+t)$$

  故可得$$\hat p = \frac{\alpha+h}{\alpha+\beta+h+t}$$

  代入具体的数据，$$\alpha=200, \beta=200, h=3, t=7$$计算可得$$\hat p = \frac{\alpha+h}{\alpha+\beta+h+t}=\frac{203}{410}=0.495$$

  我们的先验知识对结果产生了很大的影响，不添加先验时极大似然估计的结果是p=3\/10，添加先验之后，较少的实验数据只对先验做出微小的调整，贝叶斯估计的结果是𝑝 = 0.495。可以看出样本较少时先验对结果产生重要的影响，但随着样本量的增加先验的影响逐渐减弱，并且贝叶斯估计的结果趋近极大似然估计的结果。这个结论不仅仅对于硬币问题成立，对于**所有的贝叶斯估计，随着样本量的增加先验的影响逐渐减弱，贝叶斯估计趋近极大似然估计**。这个性质很像人的思维过程，人们总是根据生活现实修正原有想法，原有的想法如果和现实相一致，这些想法将得到加强，否则原有的想法会被削弱， 取而代之的是更加接近事实的想法，总之人们的想法会随着经验的积累更加贴近事实，这就是随着样本量的增加，先验的作用逐渐减弱的过程。

  总结：

  **如果样本量小，先验知识又是可获得的，贝叶斯估计能够将先验知识和样本信息整合起来获得更好的效果。**

  **如果样本量较大，先验产生的作用很小，可以忽略。贝叶斯估计趋近极大似然估计，只反应样本信息。**


# Code

```python
#Import Library
from sklearn.naive_bayes import GaussianNB
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create SVM classification object
model = GaussianNB()
# there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link
#from sklearn.naive_bayes import BernoulliNB
;clf = BernoulliNB()
#from sklearn.naive_bayes import MultinomialNB
;clf = MultinomialNB()
# Train the model using the training sets and check score
model.fit(X, y)
#Predict Output
predicted= model.predict(x_test)
```

sklearn中一共有三种模型：MultinomialNB，BernoulliNB和GaussianNB。

用法大致相同，**参数alpha=1.0（当多项式和伯努利时候启用的拉普拉斯平滑算子）；fit\_prior=False（当多项式和伯努利时候启用：是否根据现有数据计算先验概率分布P\(Y\)，如果false，表示启用统一分布；但如果给了class\_prior，那么就不会根据现有数据计算先验概率，高斯中是priors参数）**。

# Reference

* [朴素贝叶斯法](http://www.wengweitao.com/po-su-bei-xie-si-fa.html)
* [贝叶斯网络](http://clyyuanzi.github.io/2016/03/17/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/)
* [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)
* [朴素贝叶斯](https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/nb.md)
* [朴素贝叶斯及其数学推导](http://blog.csdn.net/cvrszeng/article/details/52336093)
* [极大似然估计法推出朴素贝叶斯法中的先验概率估计公式？](https://www.zhihu.com/question/33959624/answer/92311220)
* [贝叶斯估计与最大似然估计](http://qinshaoq.blog.ustc.edu.cn/?p=47)

