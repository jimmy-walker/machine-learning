# Representation

EM算法是一种迭代算法，**用于含有隐变量\(hidden variable\)的概率模型参数的极大似然估计，记住该算法意义**，或极大后验概率估计。EM算法的每次迭代由两步组成：**E步，求期望\(expectation\)；M步，求极大\( maximization \)，所以这一算法称为期望极大算法**\(expectation maximization algorithm\)，简称EM算法。

如果概率模型的变量都是观测变量，那么给定数据之后就可以直接使用极大似然法或者贝叶斯估计模型参数。但是当模型含有隐含变量的时候就不能简单的用这些方法来估计，EM就是一种含有隐含变量的概率模型参数的极大似然估计法。

1. 似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。

  **注意与最大似然估计中的似然函数是联合概率乘积不同（这是更复杂的似然函数），而这里是简单的似然函数。**

  概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

  在已知某个参数B时，事件A会发生的概率写作$$P(A|B)$$，而当未知参数B时，$$L(B|A)$$表示为已知有事件A发生时，参数B发生的似然性（“概率”）。具体解释为：似然函数$$L(b|A)=P(A|B=b)$$（数值上与条件概率相等）表示为**在事件A发生时，参数B=b的似然性**。**J看上去形式一样，但是意义不一样，记住该似然函数定义**。对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。

2. EM算法推导。

    面对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据），即极大化下式（**不完全数据Y的对数似然函数**）。

    $$L(θ)=logP(Y|θ)=log\sum_{z}P(Y,Z|θ)=log(\sum_{z}P(Y|Z,\theta)P(Z|\theta))$$

    极大化的主要困难是上式中含有未观测数据并有包含和（或积分）的对数，EM算法则是通过迭代逐步近似极大化$$L(\theta)$$的，**如果采用梯度下降等方法求导，则隐变量数目上升，无法求解，所以用迭代法**。

    

3. EM算法。

    输入：观测数据$$Y$$，隐变量数据$$Z$$，联合分布$$P(Y,Z|\theta)$$，条件分布$$P(Z|Y,\theta)$$；

    输出：模型参数$$\theta$$。

    1. 选择参数的初值$$\theta^{(0)}$$，开始迭代；

    2. E步：记$$\theta^{(0)}$$为第$$i$$次迭代参数$$\theta$$的估计值，在第$$i+1$$次迭代的E步，计算

        $$Q(\theta, \theta^{(i)}) = E_z[\log P(Y, Z|\theta) | Y, \theta^{(i)}]\\= \sum_z \log P(Y, Z | \theta) P (Z|Y, \theta^{(i)})$$
    
        这里，$$P(Z|Y, \theta^{(i)})$$是在给定观测数据Y和当前的参数估计$$\theta^{(i)}$$下隐变量数据Z的条件概率分布；

    3. 求使$$Q(\theta, \theta^{(i)})$$极大化的$$\theta$$，确定第$$i+1$$次迭代的参数的估计值$$\theta^{(i+1)}$$

        $$\theta^{(i+1)} = \arg \max_{\theta} Q(\theta, \theta^{(i)})$$

    4. 重复第(2)步和第(3)步，直到收敛。

    其中的函数$$Q(\theta, \theta^{(i)})$$是**EM算法的核心。称为Q函数**。

    Q函数：完全数据的对数似然函数$$\log P(Y, Z | \theta)$$关于在给定观测数据Y和当前参数$$\theta^{(i)}$$下对未观测数据Z的条件概率分布$$P(Z|Y, \theta^{i})$$的期望:
  
    $$Q(\theta, \theta^{(i)}) = E_Z[\log P(Y, Z|\theta) | Y, \theta^{(i)}]\\= \sum_Z P(Z | Y, \theta^{(i)}) \log P(Y, Z | \theta)$$

    算法说明：

    1. 参数的初值可以任意选择。但需注意EM算法对初值是敏感的。**常用的办法是选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的。**

    2.E步求$$Q(\theta, \theta^{(i)})$$。Q函数式中Z是未观测数据，Y是观测数据。注意，$$Q(\theta, \theta^{(i)})$$的第1个变量$$\theta$$表示要极大化的参数，第2个变量$$\theta^{(i)}$$表示参数的当前估计值。每次迭代实际在求Q函数及其极大。

    3. M步求$$Q(\theta, \theta^{(i)})$$的极大化，得到$$\theta^{(i+1)}$$，完成一次迭代$$\theta^{(i)}\rightarrow\theta^{(i+1)}$$。后面将证明每次迭代使似然函数增大或达到局部极值。

    4. 给出停止迭代的条件，一般是$$\theta$$或是Q函数更新的差值小于某一较小的正数。

3. EM算法推导k-means


# Reference

* [似然函数](https://zh.wikipedia.org/zh-hans/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)
* [EM算法](http://m.it610.com/article/3660270.htm)
* [统计学习方法 李航---第9章 EM算法及其推广](http://blog.csdn.net/demon7639/article/details/51011424)
