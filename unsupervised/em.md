# Representation
EM算法是一种迭代算法，**用于含有隐变量(hidden variable)的概率模型参数的极大似然估计，记住该算法意义**，或极大后验概率估计。EM算法的每次迭代由两步组成：**E步，求期望(expectation)；M步，求极大( maximization )，所以这一算法称为期望极大算法**(expectation maximization algorithm)，简称EM算法。 

如果概率模型的变量都是观测变量，那么给定数据之后就可以直接使用极大似然法或者贝叶斯估计模型参数。但是当模型含有隐含变量的时候就不能简单的用这些方法来估计，EM就是一种含有隐含变量的概率模型参数的极大似然估计法。

1. 似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。

    **注意与最大似然估计中的似然函数是联合概率乘积不同（这是更复杂的似然函数），而这里是简单的似然函数。**

    概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

    在已知某个参数B时，事件A会发生的概率写作$$P(A|B)$$，而当未知参数B时，$$L(B|A)$$表示为已知有事件A发生时，参数B发生的似然性（“概率”）。具体解释为：似然函数$$L(b|A)=P(A|B=b)$$（数值上与条件概率相等）表示为**在事件A发生时，参数B=b的似然性**。**J看上去形式一样，但是意义不一样，记住该似然函数定义**。对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。

2. EM算法

    输入：观测数据$$Y$$，隐变量数据$$Z$$，联合分布$$P(Y,Z|\theta)$$，条件分布$$P(Z|Y,\theta)$$；
    输出：模型参数$$\theta$$。

    1. 选择参数的初值$$\theta^{(0)}$$，开始迭代； 
    2. E步：记$$\theta^{(0)}$$为第$$i$$次迭代参数$$\theta$$的估计值，在第i+1i+1次迭代的E步，计算

3. EM算法推导k-means

# Reference
- [似然函数](https://zh.wikipedia.org/zh-hans/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)
- [EM算法](http://m.it610.com/article/3660270.htm)